{"cells":[{"cell_type":"markdown","metadata":{"id":"LDG7ojVvg5Pi"},"source":["## Vectorization 이란?"]},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l8CY7vsdhQak","executionInfo":{"status":"ok","timestamp":1667211016088,"user_tz":-540,"elapsed":6188,"user":{"displayName":"오지은","userId":"09994726054368880906"}},"outputId":"ec3ffe5a-a70e-4bee-e096-e4c5a41814e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[K     |████████████████████████████████| 19.4 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n","Collecting JPype1>=0.7.0\n","  Downloading JPype1-1.4.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n","\u001b[K     |████████████████████████████████| 465 kB 41.1 MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qviPBfnog5Pr","executionInfo":{"status":"ok","timestamp":1667211033817,"user_tz":-540,"elapsed":13549,"user":{"displayName":"오지은","userId":"09994726054368880906"}},"outputId":"1a91ab6b-d91f-40c3-be0d-94c7bde7f0ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["형태소 단위로 문장 분리\n","----------------------\n","['자연어 처리 는 정말 정말 즐거워', '즐거운 자연어 처리 다 같이 해보자']\n"]}],"source":["from konlpy.tag import Okt\n","import re\n","\n","Okt = Okt()\n","\n","sentences = ['자연어 처리는 정말 정말 즐거워.', '즐거운 자연어 처리 다같이 해보자.']\n","tokens = []\n","\n","for sentence in sentences:\n","    sentence = re.sub('[^가-힣a-z]', ' ', sentence) #간단한 전처리\n","    token = (Okt.morphs(sentence)) #형태소 분석기를 이용햔 토큰 나누기\n","    tokens.append(' '.join(token))\n","\n","print(\"형태소 단위로 문장 분리\")\n","print(\"----------------------\")\n","print(tokens)"]},{"cell_type":"markdown","metadata":{"id":"C138ALJ9g5Pz"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"V6GMTFAvg5P3"},"source":["### 1. One Hot Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wla6jOWSg5P9","executionInfo":{"status":"ok","timestamp":1667211064910,"user_tz":-540,"elapsed":6514,"user":{"displayName":"오지은","userId":"09994726054368880906"}},"outputId":"0efa3812-2eba-481e-85c7-826b14a82391"},"outputs":[{"output_type":"stream","name":"stdout","text":["각 토큰에게 고유의 정수 부여\n","----------------------\n","{'자연어': 1, '처리': 2, '정말': 3, '는': 4, '즐거워': 5, '즐거운': 6, '다': 7, '같이': 8, '해보자': 9}\n"," \n","부여된 정수로 표시된 문장1\n","----------------------\n","[1, 2, 4, 3, 3, 5]\n"," \n","부여된 정수로 표시된 문장2\n","----------------------\n","[6, 1, 2, 7, 8, 9]\n"," \n","문장1의 one-hot-encoding\n","----------------------\n","[[0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 1.]]\n"," \n","문장2의 one-hot-encoding\n","----------------------\n","[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"]}],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import to_categorical\n","\n","t = Tokenizer()\n","t.fit_on_texts(tokens)\n","print(\"각 토큰에게 고유의 정수 부여\")\n","print(\"----------------------\")\n","print(t.word_index) \n","print(\" \")\n","\n","s1=t.texts_to_sequences(tokens)[0] \n","print(\"부여된 정수로 표시된 문장1\")\n","print(\"----------------------\")\n","print(s1)\n","print(\" \")\n","\n","s2=t.texts_to_sequences(tokens)[1]\n","print(\"부여된 정수로 표시된 문장2\")\n","print(\"----------------------\")\n","print(s2)\n","print(\" \")\n","\n","s1_one_hot = to_categorical(s1)\n","print(\"문장1의 one-hot-encoding\")\n","print(\"----------------------\")\n","print(s1_one_hot)\n","print(\" \")\n","\n","s2_one_hot = to_categorical(s2)\n","print(\"문장2의 one-hot-encoding\")\n","print(\"----------------------\")\n","print(s2_one_hot)"]},{"cell_type":"markdown","metadata":{"id":"sriKR6Eeg5QH"},"source":["## 2.Count vectorization"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JnCeRV6Ng5QJ","executionInfo":{"status":"ok","timestamp":1667211072147,"user_tz":-540,"elapsed":1718,"user":{"displayName":"오지은","userId":"09994726054368880906"}},"outputId":"87f3bf59-e289-44c6-d350-8424ee36f7ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["['같이', '자연어', '정말', '즐거운', '즐거워', '처리', '해보자']\n","[[0 1 2 0 1 1 0]\n"," [1 1 0 1 0 1 1]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","vectorizer = CountVectorizer()\n","vectors = vectorizer.fit_transform(tokens) #여러 개의 문장을 넣어줘야 작동합니다!!\n","\n","print(vectorizer.get_feature_names())\n","print(vectors.toarray())"]},{"cell_type":"markdown","metadata":{"id":"1pX6zb5mg5Qc"},"source":["## 3. TfIdf"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9YLAJWw6g5Qh","executionInfo":{"status":"ok","timestamp":1667211082180,"user_tz":-540,"elapsed":412,"user":{"displayName":"오지은","userId":"09994726054368880906"}},"outputId":"4bea0b43-262c-43a5-c3e0-93a1b7f54935"},"outputs":[{"output_type":"stream","name":"stdout","text":["['같이', '자연어', '정말', '즐거운', '즐거워', '처리', '해보자']\n","[[0.         0.29017021 0.81564821 0.         0.4078241  0.29017021\n","  0.        ]\n"," [0.49922133 0.35520009 0.         0.49922133 0.         0.35520009\n","  0.49922133]]\n"]}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf = TfidfVectorizer(min_df=0)\n","tfidf_vectorizer = tfidf.fit_transform(tokens) \n","\n","#tf-idf dictionary    \n","tfidf_dict = tfidf.get_feature_names()\n","print(tfidf_dict)\n","print(tfidf_vectorizer.toarray())"]},{"cell_type":"markdown","metadata":{"id":"KYxpSB9Lg5Q3"},"source":["## 5. 대회 적용"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"Z4yaasr5g5Q4","executionInfo":{"status":"error","timestamp":1667211100055,"user_tz":-540,"elapsed":718,"user":{"displayName":"오지은","userId":"09994726054368880906"}},"outputId":"b58f7b71-6158-4c48-c04f-aef77e98cc03"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-a9fe4d57a4b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext2sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"]}],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","def text2sequence(train_text, max_len=100):\n","    \n","    tokenizer = Tokenizer() #keras의 vectorizing 함수 호출\n","    tokenizer.fit_on_texts(train_text) #train 문장에 fit\n","    train_X_seq = tokenizer.texts_to_sequences(train_text) #각 토큰들에 정수 부여\n","    vocab_size = len(tokenizer.word_index) + 1 #모델에 알려줄 vocabulary의 크기 계산\n","    print('vocab_size : ', vocab_size)\n","    X_train = pad_sequences(train_X_seq, maxlen = max_len) #설정한 문장의 최대 길이만큼 padding\n","    \n","    return X_train, vocab_size, tokenizer\n","\n","train_X, vocab_size, vectorizer = text2sequence(train['text'], max_len = 100)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}